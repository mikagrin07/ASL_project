{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mounts Google Drive and sets up tools to display videos in Colab."
      ],
      "metadata": {
        "id": "TD27DVNuvqR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe==0.10.9 --quiet\n",
        "!pip install moviepy tensorflow pandas opencv-python --quiet"
      ],
      "metadata": {
        "id": "hlEBOeUFbX_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b08PeZqzqzAp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set values"
      ],
      "metadata": {
        "id": "53c99hiav_3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "LEFT_HAND_LANDMARKS = 21\n",
        "RIGHT_HAND_LANDMARKS = 21\n",
        "POSE_LANDMARKS = 33\n",
        "FEATURES_PER_FRAME = (LEFT_HAND_LANDMARKS + RIGHT_HAND_LANDMARKS + POSE_LANDMARKS) * 2  # X, Y for each landmark\n",
        "MAX_FRAMES = 15  # Number of frames used for classification\n",
        "MODEL_PATH = \"/content/drive/MyDrive/ASL_project_GDrive/Models/Copy of asl_lstm_best_curr_epoch_noOther90.h5\"\n",
        "CLASS_NAMES = ['book', 'computer_bk', 'drink', 'i', 'other', 'read', 'science', 'study', 'water']\n"
      ],
      "metadata": {
        "id": "Tu1jNTkfrOdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict and draw landmarks on video"
      ],
      "metadata": {
        "id": "s0EL99giwCMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load MediaPipe models"
      ],
      "metadata": {
        "id": "7BANSSsy2aIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\n",
        "!wget -O hand_landmarker.task -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ],
      "metadata": {
        "id": "htm6xIF1mP_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads MediaPipeâ€™s hand and pose landmark models from .task files, enabling detection of two hands and full-body pose landmarks."
      ],
      "metadata": {
        "id": "XNW8p_E22qok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the hand landmark model from the .task file\n",
        "base_options_hand = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "# Allow detection of up to 2 hands in each frame\n",
        "options_hand = vision.HandLandmarkerOptions(base_options=base_options_hand, num_hands=2)\n",
        "# Create the hand landmark detector\n",
        "hand_landmarker = vision.HandLandmarker.create_from_options(options_hand)\n",
        "\n",
        "# Initialize the pose landmark model from the .task file\n",
        "base_options_pose = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "# Use default pose detection options\n",
        "options_pose = vision.PoseLandmarkerOptions(base_options=base_options_pose)\n",
        "# Create the pose landmark detector\n",
        "pose_landmarker = vision.PoseLandmarker.create_from_options(options_pose)"
      ],
      "metadata": {
        "id": "a2c42UQwmTq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "kVmJi6Hb2swg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(MODEL_PATH)"
      ],
      "metadata": {
        "id": "0DNmpL-SmXHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts up to MAX_FRAMES evenly spaced frames from a video and returns a matrix of (x, y) pose and hand landmarks for each frame."
      ],
      "metadata": {
        "id": "ox6G1ajf2-Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_landmark_matrix_full_video(video_path):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video properties: frame rate, width, height\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"total_frames: '{total_frames}'\")\n",
        "\n",
        "    # Choose how many frames to extract (limited to MAX_FRAMES)\n",
        "    n_frames = min(total_frames, MAX_FRAMES)\n",
        "    print(f\"n_frames: '{n_frames}'\")\n",
        "\n",
        "    # Generate evenly spaced frame indices to sample from the video\n",
        "    selected_indices = np.linspace(0, total_frames - 1, n_frames, dtype=int)\n",
        "\n",
        "    frames = []\n",
        "    current_frame = 0\n",
        "    selected_set = set(selected_indices)  # Convert to set for fast lookup\n",
        "\n",
        "    # Read frames one by one\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or len(frames) >= n_frames:\n",
        "            break\n",
        "\n",
        "        # Process only the selected frames\n",
        "        if current_frame in selected_set:\n",
        "            print(f\"current_frame: '{current_frame}'\")\n",
        "\n",
        "            # Convert BGR to RGB (MediaPipe expects RGB)\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
        "\n",
        "            # Detect hand and pose landmarks\n",
        "            hand_result = hand_landmarker.detect(mp_image)\n",
        "            pose_result = pose_landmarker.detect(mp_image)\n",
        "\n",
        "            # Initialize frame data with zeros\n",
        "            frame_data = [0.0] * FEATURES_PER_FRAME\n",
        "\n",
        "            # Insert hand landmarks (if detected)\n",
        "            if hand_result.hand_landmarks:\n",
        "                for idx, hand in enumerate(hand_result.hand_landmarks):\n",
        "                    # Determine if left or right hand\n",
        "                    base = 0 if hand_result.handedness[idx][0].category_name == \"Left\" else 21 * 2\n",
        "                    for lm_idx, lm in enumerate(hand):\n",
        "                        frame_data[base + lm_idx * 2] = lm.x\n",
        "                        frame_data[base + lm_idx * 2 + 1] = lm.y\n",
        "\n",
        "            # Insert pose landmarks (if detected)\n",
        "            if pose_result.pose_landmarks:\n",
        "                base = (21 + 21) * 2  # Offset after both hands\n",
        "                for lm_idx, lm in enumerate(pose_result.pose_landmarks[0]):\n",
        "                    frame_data[base + lm_idx * 2] = lm.x\n",
        "                    frame_data[base + lm_idx * 2 + 1] = lm.y\n",
        "\n",
        "            # Save the extracted landmarks for this frame\n",
        "            frames.append(frame_data)\n",
        "\n",
        "        current_frame += 1\n",
        "\n",
        "    # Pad the sequence if fewer than n_frames were extracted\n",
        "    while len(frames) < n_frames:\n",
        "        frames.append([0.0] * FEATURES_PER_FRAME)\n",
        "\n",
        "    # Convert to numpy array and return\n",
        "    frames = np.array(frames[:n_frames])\n",
        "    return frames"
      ],
      "metadata": {
        "id": "fIV8rXromdXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicts the ASL word in a video by extracting landmarks, running the model, and returning the predicted class name."
      ],
      "metadata": {
        "id": "zZPNPKHL3EFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_video_full(video_path):\n",
        "    # Print which video is being processed\n",
        "    print(f\"Processing video: {video_path}\")\n",
        "\n",
        "    # Extract the (frames x features) landmark matrix from the video\n",
        "    matrix = extract_landmark_matrix_full_video(video_path)\n",
        "\n",
        "    # Add a batch dimension to match the model's input shape (1, frames, features)\n",
        "    matrix = np.expand_dims(matrix, axis=0)\n",
        "\n",
        "    # Run the model to get prediction probabilities\n",
        "    pred = model.predict(matrix)\n",
        "\n",
        "    # Get the index of the highest probability (predicted class)\n",
        "    label = np.argmax(pred, axis=1)[0]\n",
        "\n",
        "    # Map the index to the actual word label\n",
        "    word = CLASS_NAMES[label]\n",
        "\n",
        "    # Print the predicted word\n",
        "    print(f\"Prediction: '{word}'\")\n",
        "\n",
        "    return word"
      ],
      "metadata": {
        "id": "By3N4NXumgMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run prediction"
      ],
      "metadata": {
        "id": "pGZSwtoF3oRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/ASL_Project_Mika/video_examples/book.mp4\" #can be any sort of video (you can choose from the \"video examples\" folder or upload yourself)\n",
        "predict_video_full(video_path)"
      ],
      "metadata": {
        "id": "s9dSgU1W5doo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}