{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " 专抓 转 拽 砖 专 转 拽爪  砖爪专驻转 砖  专"
      ],
      "metadata": {
        "id": "LxOvm0w83Lmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounts Google Drive and sets up tools to display videos in Colab."
      ],
      "metadata": {
        "id": "TD27DVNuvqR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b08PeZqzqzAp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access dataset and save/load models\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports tools for building and evaluating an LSTM classification model."
      ],
      "metadata": {
        "id": "BqeLClGmv4jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "from tensorflow.keras.models import load_model\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "e8o_Y1Ghq94E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set paths and labels"
      ],
      "metadata": {
        "id": "jg3mi7QaV3DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_train_path = \"/content/drive/MyDrive/ASL_Project_Mika/Dataset_submission/annotated_train_data\"\n",
        "augmented_train_path = \"/content/drive/MyDrive/ASL_Project_Mika/Dataset_submission/augmented_train_data\"\n",
        "test_path = \"/content/drive/MyDrive/ASL_Project_Mika/Dataset_submission/annotated_test_data\"\n",
        "allowed_labels = {'book', 'drink', 'computer', 'study','science','i', 'water', 'read', 'other'}  # Only use these classes"
      ],
      "metadata": {
        "id": "c_KtIKX-rDG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set fixed values"
      ],
      "metadata": {
        "id": "53c99hiav_3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_FRAMES = 30  # Fixed sequence length\n",
        "FEATURE_SIZE = 150  # Fixed feature size\n",
        "LEARNING_RATE = 0.001  # Custom learning rate"
      ],
      "metadata": {
        "id": "Tu1jNTkfrOdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Argumantion testing"
      ],
      "metadata": {
        "id": "qG0uDGtb5TJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to load sequences and labels"
      ],
      "metadata": {
        "id": "s0EL99giwCMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sequences_from_multiple_paths(folders, max_per_class=150):\n",
        "    sequences = []  # List to store all sequences (each is a (30, FEATURE_SIZE) matrix)\n",
        "    labels = []     # List to store corresponding class labels\n",
        "    label_encoder = None  # Will be used to encode class names as numeric labels\n",
        "\n",
        "    # Collect all class names from allowed folders\n",
        "    class_names_set = set()\n",
        "    for folder in folders:\n",
        "        # Only include subfolder names that are in allowed_labels\n",
        "        class_names_set.update(name for name in os.listdir(folder) if name in allowed_labels)\n",
        "\n",
        "    # Sort class names for consistent label encoding\n",
        "    class_names = sorted(list(class_names_set))\n",
        "    print(f\"Class names: {class_names}\")\n",
        "\n",
        "    # Initialize and fit the label encoder on the collected class names\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(class_names)\n",
        "\n",
        "    # Iterate over each folder (e.g., train and test folders)\n",
        "    for folder in folders:\n",
        "        # For each valid class name (folder)\n",
        "        for class_name in class_names:\n",
        "            class_path = os.path.join(folder, class_name) # Full path to class folder\n",
        "            if os.path.isdir(class_path): # Proceed only if it's a directory\n",
        "                files = [f for f in os.listdir(class_path) if f.endswith(\".csv\")]\n",
        "                files = sorted(files)[:max_per_class]  # Limit per class per folder\n",
        "\n",
        "                for filename in files:\n",
        "                    file_path = os.path.join(class_path, filename)\n",
        "                    try:\n",
        "                        # Load CSV file as DataFrame\n",
        "                        df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "                        # Ensure feature size matches expected\n",
        "                        if df.shape[1] < FEATURE_SIZE:\n",
        "                            # Pad columns with zeros if there are fewer features\n",
        "                            padding = np.zeros((df.shape[0], FEATURE_SIZE - df.shape[1]))\n",
        "                            df = pd.concat([df, pd.DataFrame(padding)], axis=1)\n",
        "                        elif df.shape[1] > FEATURE_SIZE:\n",
        "                            # Trim excess features\n",
        "                            df = df.iloc[:, :FEATURE_SIZE]\n",
        "\n",
        "                        # Select up to MAX_FRAMES from the sequence, evenly spaced\n",
        "                        total_frames = df.shape[0]\n",
        "                        step = max(1, total_frames // MAX_FRAMES)\n",
        "                        selected_frames = df.iloc[::step].values[:MAX_FRAMES]\n",
        "\n",
        "                        # If there are not enough frames, pad with zeros\n",
        "                        if selected_frames.shape[0] < MAX_FRAMES:\n",
        "                            padding = np.zeros((MAX_FRAMES - selected_frames.shape[0], FEATURE_SIZE))\n",
        "                            selected_frames = np.vstack([selected_frames, padding])\n",
        "\n",
        "                        # Save processed sequence and corresponding label\n",
        "                        sequences.append(selected_frames)\n",
        "                        labels.append(label_encoder.transform([class_name])[0])\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {file_path}: {e}\")\n",
        "                        continue\n",
        "\n",
        "    # Convert final sequence and label lists to NumPy arrays\n",
        "    X = np.array(sequences)  # Shape: (num_samples, MAX_FRAMES, FEATURE_SIZE)\n",
        "    y = np.array(labels)     # Shape: (num_samples,)\n",
        "\n",
        "    print(f\" Loaded combined dataset: X={X.shape}, y={y.shape}\")\n",
        "    return X, y, label_encoder"
      ],
      "metadata": {
        "id": "p452a-7SrPfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load train and test datasets"
      ],
      "metadata": {
        "id": "33kMnZLbwFd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, label_encoder = load_sequences_from_multiple_paths([annotated_train_path, augmented_train_path], max_per_class=150)\n",
        "X_test, y_test, _ = load_sequences_from_multiple_paths([test_path], max_per_class=30)\n",
        "\n",
        "# Check if dataset is loaded properly\n",
        "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
        "    raise ValueError(\"No valid training or testing data found. Please check dataset format.\")"
      ],
      "metadata": {
        "id": "Nij7tM9krUo4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define an LSTM-based sequence classification model"
      ],
      "metadata": {
        "id": "3Dylu5YlwJIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    # Mask padded frames\n",
        "    Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "\n",
        "    # First LSTM layer (return sequences for stacking)\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.3),                         # Dropout after first LSTM\n",
        "    BatchNormalization(),                # Batch norm helps stabilize deeper LSTM\n",
        "\n",
        "    # Second LSTM layer (final sequence output)\n",
        "    LSTM(64),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Dense hidden layer\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),                         # Optional dropout before final output\n",
        "\n",
        "    # Output layer\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "jmArNmTyraTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set custom learning rate and compile the model with the custom learning rat"
      ],
      "metadata": {
        "id": "Iq_iOYL1wLzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lxLP4vUQrbC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Track the best epoch for confusion matrix"
      ],
      "metadata": {
        "id": "8YTFDlfCwb40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch = 0\n",
        "best_val_accuracy = 0.0\n",
        "best_y_pred = None"
      ],
      "metadata": {
        "id": "KDxXOBhstHbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom callback to track precision, recall, and store the best model\n",
        "predictions based on validation accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "ETUNIKLawfFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrecisionRecallCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Use global variables to keep track of the best performing epoch\n",
        "        global best_epoch, best_val_accuracy, best_y_pred\n",
        "\n",
        "        # Get model predictions for training and test sets\n",
        "        y_pred_train = np.argmax(self.model.predict(X_train), axis=1)\n",
        "        y_pred_test = np.argmax(self.model.predict(X_test), axis=1)\n",
        "\n",
        "        # Compute precision and recall for training data\n",
        "        train_precision = precision_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
        "        train_recall = recall_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
        "\n",
        "        # Compute precision and recall for test (validation) data\n",
        "        test_precision = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "        test_recall = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "\n",
        "        # Get current epoch's validation accuracy (automatically logged by Keras)\n",
        "        val_accuracy = logs[\"val_accuracy\"]\n",
        "\n",
        "        # Print summary of precision and recall for this epoch\n",
        "        print(f\"\\n Epoch {epoch+1}: Train Precision={train_precision:.4f}, Train Recall={train_recall:.4f}, \"\n",
        "              f\"Test Precision={test_precision:.4f}, Test Recall={test_recall:.4f}\")\n",
        "\n",
        "        # Track best epoch\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy   # Update the best accuracy\n",
        "            best_epoch = epoch + 1             # Save the best epoch (1-indexed)\n",
        "            best_y_pred = y_pred_test.copy()   # Save the best predictions"
      ],
      "metadata": {
        "id": "b84teJ2mtILO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a callback to save the model to Google Drive whenever validation accuracy improves during training."
      ],
      "metadata": {
        "id": "PrufGTTH8Oh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    \"/content/drive/MyDrive/ASL_Project_Mika/Model/ASL_final_model.h5\",\n",
        "    monitor='val_accuracy',  # Track validation accuracy during training\n",
        "    save_best_only=True,     # Only save the model if val_accuracy improves\n",
        "    save_weights_only=False, # Save the entire model (not just weights)\n",
        "    verbose=1                # Print a message each time the model is saved\n",
        ")"
      ],
      "metadata": {
        "id": "oKn21Hr2C-hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and save model"
      ],
      "metadata": {
        "id": "XnBRPeA6wjRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model and save history\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=40,\n",
        "    batch_size=16,\n",
        "    callbacks=[PrecisionRecallCallback(), checkpoint_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "vb9ggHBNtL7o",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate best epoch with confusion matrix"
      ],
      "metadata": {
        "id": "TNPxajoqwpN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n Best Epoch: {best_epoch}, Best Validation Accuracy: {best_val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "wwzSs4MttRvP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute confusion matrix"
      ],
      "metadata": {
        "id": "ePIVMigYwq3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, best_y_pred)\n",
        "\n",
        "# Plot with adjustments\n",
        "plt.figure(figsize=(10, 8))  # Larger figure for spacing\n",
        "sns.heatmap(conf_matrix,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_,\n",
        "            cmap='Blues')\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "# Rotate the tick labels for clarity\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()  # Prevent clipping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dW2_lcyptSTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report"
      ],
      "metadata": {
        "id": "C2WwvXZ-Q6Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test, best_y_pred, target_names=label_encoder.classes_, digits=4)\n",
        "\n",
        "print(\"\\n Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "S-9V6jDSQ20j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plots the training and validation accuracy over epochs to visualize the models learning performance."
      ],
      "metadata": {
        "id": "io0QM4X87v4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy', color='lightpink')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='lightblue')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cpC3dtzbD4tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plots the training and validation loss over epochs to track how well the model is minimizing error."
      ],
      "metadata": {
        "id": "Hw-9M82q71eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='lightpink')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss', color='lightblue')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BNj9DH044NkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the trained model, predicts on the test set, calculates precision and recall per class, and visualizes them in a bar chart."
      ],
      "metadata": {
        "id": "ohR-uVS58A1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load model (adjust path if needed) ---\n",
        "model = load_model('/content/drive/MyDrive/ASL_Project_Mika/Model/ASL_final_model.h5')  # or .h5\n",
        "\n",
        "# --- Predict on test set ---\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# --- Calculate precision and recall per class ---\n",
        "precision = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
        "\n",
        "# --- Get class labels ---\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "# --- Plot ---\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.35  # Bar width\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(x - width/2, precision, width, label='Precision', color='#ffb6c1')  # Light pink\n",
        "plt.bar(x + width/2, recall, width, label='Recall', color='#add8e6')        # Light blue\n",
        "\n",
        "plt.xlabel('ASL Word')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision and Recall per ASL Class')\n",
        "plt.xticks(x, class_names, rotation=45)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pbEKd6I_DCTR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}