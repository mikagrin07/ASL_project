{"cells":[{"cell_type":"markdown","source":[" ---  Install dependencies ---"],"metadata":{"id":"GTfQSFPDr5Rr"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10274,"status":"ok","timestamp":1747678425131,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"},"user_tz":-180},"id":"MycDHutguDJv","outputId":"99339be2-0c55-4a4d-e8ea-40ca44d75eb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n"]}],"source":["!pip install --upgrade pip\n","!pip install mediapipe==0.10.9 --quiet\n","!pip install moviepy tensorflow pandas --quiet\n","!pip install opencv-python-headless"]},{"cell_type":"markdown","source":["Mounts Google Drive and sets up tools to display videos in Colab."],"metadata":{"id":"X0lFmxeXsOEv"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3011,"status":"ok","timestamp":1747678428151,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"},"user_tz":-180},"id":"H5AeKiZPuTUn","outputId":"5073b0b5-1b28-49d7-d217-a87373c9233b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import requests\n","from IPython.display import Video, display\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"lPjVECjmsHZ-"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from moviepy.video.io.VideoFileClip import VideoFileClip\n","import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","from tensorflow.keras.models import load_model"],"metadata":{"id":"B4AwRnauoo6P","executionInfo":{"status":"ok","timestamp":1747678437282,"user_tz":-180,"elapsed":9124,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":[" Constants"],"metadata":{"id":"kBhKSyansSbL"}},{"cell_type":"code","source":["FEATURES_PER_FRAME = (21 + 21 + 33) * 2  # 75 points * 2 (x, y)\n","MAX_FRAMES = 30\n","MODEL_PATH = \"/content/drive/MyDrive/ASL_Project_Mika/Model/ASL_final_model.h5\"\n","class_names = ['book', 'computer', 'drink', 'i', 'other', 'read', 'science', 'study', 'water']"],"metadata":{"id":"-0jKBzb4orEz","executionInfo":{"status":"ok","timestamp":1747678437298,"user_tz":-180,"elapsed":5,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Kx1Dy2vWv0BP"}},{"cell_type":"markdown","source":["Load MediaPipe models"],"metadata":{"id":"oc3aqPH7sUoa"}},{"cell_type":"code","source":["!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\n","!wget -O hand_landmarker.task -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"],"metadata":{"id":"ClKqoHghpohg","executionInfo":{"status":"ok","timestamp":1747678441588,"user_tz":-180,"elapsed":4279,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":[" Create hand and pose landmark detectors"],"metadata":{"id":"-Pz4Y3o6sdme"}},{"cell_type":"code","source":["base_options_hand = python.BaseOptions(model_asset_path='hand_landmarker.task')\n","options_hand = vision.HandLandmarkerOptions(base_options=base_options_hand, num_hands=2)\n","hand_landmarker = vision.HandLandmarker.create_from_options(options_hand)\n","\n","base_options_pose = python.BaseOptions(model_asset_path='pose_landmarker.task')\n","options_pose = vision.PoseLandmarkerOptions(base_options=base_options_pose)\n","pose_landmarker = vision.PoseLandmarker.create_from_options(options_pose)"],"metadata":{"id":"aJLwmZjCpq57","executionInfo":{"status":"ok","timestamp":1747678442701,"user_tz":-180,"elapsed":780,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Load trained Keras model"],"metadata":{"id":"RRJFnd2EsjMH"}},{"cell_type":"code","source":["model = load_model(MODEL_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDNcHrbQptJ-","executionInfo":{"status":"ok","timestamp":1747678442845,"user_tz":-180,"elapsed":137,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}},"outputId":"acedffe4-c3c0-41c9-da52-e4ae6954ccc7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"markdown","source":["Extract landmark matrix from a video chunk"],"metadata":{"id":"RHYxB8xKspE7"}},{"cell_type":"code","source":["def extract_landmark_matrix(video_path):\n","    # Open the video file for reading\n","    cap = cv2.VideoCapture(video_path)\n","\n","    # Get the total number of frames in the video\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    # Select 15 frame indices evenly spaced throughout the video\n","    # This ensures the sampled frames represent the entire temporal span of the video\n","    selected_indices = np.linspace(0, total_frames - 1, 15, dtype=int)\n","\n","    frames = []  # Will hold the final landmark data per frame\n","    current_frame = 0  # Index to track current frame number during iteration\n","    selected_set = set(selected_indices)  # For fast lookup of target frames\n","\n","    # Loop over the video frames\n","    while cap.isOpened():\n","        ret, frame = cap.read()  # Read the next frame\n","        if not ret or len(frames) >= 10:\n","            # Stop reading if video ends or if we already processed 10 frames\n","            break\n","\n","        if current_frame in selected_set:\n","            # Convert frame color from BGR (OpenCV default) to RGB\n","            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","            # Wrap image in MediaPipe's format\n","            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n","\n","            # Detect hand and pose landmarks using MediaPipe models\n","            hand_result = hand_landmarker.detect(mp_image)\n","            pose_result = pose_landmarker.detect(mp_image)\n","\n","            # Initialize a frame vector filled with zeros (75 * 2 = 150 features)\n","            # This accounts for left hand (21), right hand (21), and pose (33)\n","            frame_data = [0.0] * FEATURES_PER_FRAME\n","\n","            # Process detected hand landmarks\n","            if hand_result.hand_landmarks:\n","                for idx, hand in enumerate(hand_result.hand_landmarks):\n","                    # Identify which hand it is (left or right) using handedness\n","                    # Left hand starts at index 0, right hand starts at index 42\n","                    base = 0 if hand_result.handedness[idx][0].category_name == \"Left\" else 21 * 2\n","                    for lm_idx, lm in enumerate(hand):\n","                        # Store x and y coordinates at the correct position\n","                        frame_data[base + lm_idx * 2] = lm.x\n","                        frame_data[base + lm_idx * 2 + 1] = lm.y\n","\n","            # Process detected pose landmarks\n","            if pose_result.pose_landmarks:\n","                base = (21 + 21) * 2  # Pose landmarks start after both hands\n","                for lm_idx, lm in enumerate(pose_result.pose_landmarks[0]):\n","                    # Store x and y pose coordinates\n","                    frame_data[base + lm_idx * 2] = lm.x\n","                    frame_data[base + lm_idx * 2 + 1] = lm.y\n","\n","            # Add this frame's data to the sequence\n","            frames.append(frame_data)\n","\n","        current_frame += 1  # Move to the next frame\n","\n","    cap.release()  # Release the video file\n","\n","    # If we have fewer than MAX_FRAMES, pad the list with empty frames (all zeros)\n","    while len(frames) < MAX_FRAMES:\n","        frames.append([0.0] * FEATURES_PER_FRAME)\n","\n","    # Trim to MAX_FRAMES if necessary and convert to NumPy array\n","    frames = np.array(frames[:MAX_FRAMES])\n","    return frames  # Shape: (MAX_FRAMES, 150)"],"metadata":{"id":"vmbim70NpwcG","executionInfo":{"status":"ok","timestamp":1747678442856,"user_tz":-180,"elapsed":5,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Split input video into chunks"],"metadata":{"id":"AdxVRIeVtsCn"}},{"cell_type":"code","source":["def split_video_chunks(video_path, chunk_duration):\n","    # Load the video using MoviePy\n","    video = VideoFileClip(video_path)\n","\n","    # Get the total duration of the video in seconds\n","    duration = video.duration\n","\n","    # List to store the paths of generated video chunks\n","    chunks = []\n","\n","    start = 0.0  # Starting timestamp (in seconds) of the first chunk\n","    idx = 0      # Index to number each chunk uniquely\n","\n","    # Loop to split the video until the end is reached\n","    while start < duration:\n","        # Calculate the end timestamp of the current chunk\n","        # Make sure not to exceed the video's actual duration\n","        end = min(start + chunk_duration, duration)\n","\n","        # Define output file name for this chunk (e.g., chunk_0_70.mp4 for 0.7s chunks)\n","        output_path = f\"/content/chunk_{idx}_{int(chunk_duration * 100)}.mp4\"\n","\n","        # Extract subclip and write it to disk\n","        video.subclip(start, end).write_videofile(\n","            output_path,\n","            codec=\"libx264\",       # Use H.264 codec for compatibility\n","            audio=False,           # No need to save audio\n","            verbose=False,         # Suppress detailed logs\n","            logger=None            # Prevent MoviePy from printing output\n","        )\n","\n","        # Add the chunk path to the result list\n","        chunks.append(output_path)\n","\n","        # Move to the next chunk\n","        start += chunk_duration\n","        idx += 1\n","\n","    # Return the list of all chunk file paths\n","    return chunks"],"metadata":{"id":"KOIVWei8pz6a","executionInfo":{"status":"ok","timestamp":1747678442871,"user_tz":-180,"elapsed":6,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Predict from video chunks function"],"metadata":{"id":"8U8CtiNvt1LD"}},{"cell_type":"code","source":["def predict_chunks(chunks, class_names, run_label=\"\"):\n","    predictions = []  # List to store the predicted class for each chunk\n","\n","    # Iterate over all video chunks\n","    for i, chunk in enumerate(chunks):\n","        # Extract the landmark sequence (shape: [MAX_FRAMES, 150]) from the video chunk\n","        matrix = extract_landmark_matrix(chunk)\n","\n","        # Add batch dimension so the shape becomes (1, MAX_FRAMES, 150), as expected by the model\n","        matrix = np.expand_dims(matrix, axis=0)\n","\n","        # Use the trained model to predict the class probabilities\n","        pred = model.predict(matrix, verbose=0)\n","\n","        # Get the index of the class with the highest probability\n","        label = np.argmax(pred, axis=1)[0]\n","\n","        # Extract the confidence value (max softmax score)\n","        confidence = float(np.max(pred))\n","\n","        # Get the class name using the label index\n","        word = class_names[label]\n","\n","        # Print the result in a readable format\n","        print(f\"{run_label} matrix {i+1}: '{word}' (confidence: {confidence:.2%})\")\n","\n","        # Store the predicted word\n","        predictions.append(word)\n","\n","    # Return the full list of predicted words (one per chunk)\n","    return predictions"],"metadata":{"id":"OJHzWXKep32p","executionInfo":{"status":"ok","timestamp":1747678442921,"user_tz":-180,"elapsed":41,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Merge two prediction runs smartly based on timing function"],"metadata":{"id":"LJgyMq5pt_Na"}},{"cell_type":"code","source":["def merge_predictions_with_time_v2(preds_07, preds_09, chunk_duration_07=0.7, chunk_duration_09=0.9, max_time_diff=1):\n","    final_sentence = []  # Final list to store the merged predicted words\n","    max_len = max(len(preds_07), len(preds_09))  # Determine how many positions to iterate over\n","\n","    for i in range(max_len):\n","        # Get the predicted word at index i from both lists, if available\n","        word_07 = preds_07[i] if i < len(preds_07) else None\n","        word_09 = preds_09[i] if i < len(preds_09) else None\n","\n","        # Remove 'other' class since it is not informative for final prediction\n","        if word_07 == 'other': word_07 = None\n","        if word_09 == 'other': word_09 = None\n","\n","        chosen = None  # The final word chosen for this position\n","\n","        # Case 1: Both predictions exist and are the same\n","        if word_07 and word_07 == word_09:\n","            chosen = word_07\n","\n","        # Case 2: Both exist but are different, choose based on timestamp closeness\n","        elif word_07 and word_09:\n","            time_07 = i * chunk_duration_07\n","            time_09 = i * chunk_duration_09\n","            time_diff = abs(time_07 - time_09)\n","\n","            if time_diff <= max_time_diff:\n","                chosen = word_09\n","            else:\n","                chosen = word_07\n","\n","        # Case 3: Only word_07 exists, check for temporal proximity in preds_09\n","        elif word_07:\n","            idx_in_09 = None\n","            for j in range(i, len(preds_09)):\n","                if preds_09[j] == word_07:\n","                    idx_in_09 = j\n","                    break\n","\n","            if idx_in_09 is not None:\n","                time_09 = idx_in_09 * chunk_duration_09\n","                time_07 = i * chunk_duration_07\n","                time_diff = abs(time_07 - time_09)\n","\n","                if time_diff <= max_time_diff:\n","                    chosen = word_07\n","\n","        # Case 4: Only word_09 exists, check for temporal proximity in preds_07\n","        elif word_09:\n","            idx_in_07 = None\n","            for j in range(i, len(preds_07)):\n","                if preds_07[j] == word_09:\n","                    idx_in_07 = j\n","                    break\n","\n","            if idx_in_07 is not None:\n","                time_07 = idx_in_07 * chunk_duration_07\n","                time_09 = i * chunk_duration_09\n","                time_diff = abs(time_07 - time_09)\n","\n","                if time_diff <= max_time_diff:\n","                    chosen = word_09\n","\n","        # Avoid adding repeated words consecutively\n","        if chosen and (len(final_sentence) == 0 or final_sentence[-1] != chosen):\n","            final_sentence.append(chosen)\n","\n","    return final_sentence"],"metadata":{"id":"xBdgDO2Op9fE","executionInfo":{"status":"ok","timestamp":1747678442944,"user_tz":-180,"elapsed":13,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Final master function"],"metadata":{"id":"6kYng3UiuVJR"}},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1747678442989,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"},"user_tz":-180},"id":"6iLYIppYuqpS"},"outputs":[],"source":["def dual_run_prediction_with_time_v2(video_path, class_names):\n","    # First prediction pass using 0.7-second chunks\n","    print(\"First pass (0.7s chunks):\")\n","    chunks_07 = split_video_chunks(video_path, chunk_duration=0.7)\n","    preds_07 = predict_chunks(chunks_07, class_names, run_label=\"0.7s\")\n","\n","    # Second prediction pass using 0.9-second chunks\n","    print(\"\\nSecond pass (0.9s chunks):\")\n","    chunks_09 = split_video_chunks(video_path, chunk_duration=0.9)\n","    preds_09 = predict_chunks(chunks_09, class_names, run_label=\"0.9s\")\n","\n","    # Merge the two prediction passes to improve accuracy and robustness\n","    final_sentence = merge_predictions_with_time_v2(preds_07, preds_09)\n","\n","    # Display the intermediate and final results\n","    print(\"\\nFull sentence (0.7s chunks):\", \" \".join(preds_07))\n","    print(\"Full sentence (0.9s chunks):\", \" \".join(preds_09))\n","    print(\"Final combined sentence:\", \" \".join([w for w in final_sentence if w]))"]},{"cell_type":"markdown","source":["Execute the dual-pass prediction function"],"metadata":{"id":"DpiRf9ifv7Bs"}},{"cell_type":"code","source":["# Run the full dual-pass prediction pipeline on a sample video\n","video_path = \"/content/drive/MyDrive/ASL_Project_Mika/sentences_examples/first_test.mp4\"\n","dual_run_prediction_with_time_v2(video_path, class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejm0mQ6OvpkW","executionInfo":{"status":"ok","timestamp":1747678520738,"user_tz":-180,"elapsed":77740,"user":{"displayName":"Mika Grinberg","userId":"06597200071291558479"}},"outputId":"5f2dee42-dd02-49e0-cdad-31d2b665a089"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["First pass (0.7s chunks):\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/ASL_project_GDrive/Dataset/sentences_tests1/first_test.mp4, 1221120 bytes wanted but 0 bytes read,at frame 440/442, at time 7.33/7.36 sec. Using the last valid frame instead.\n","  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n","\n","WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/ASL_project_GDrive/Dataset/sentences_tests1/first_test.mp4, 1221120 bytes wanted but 0 bytes read,at frame 441/442, at time 7.35/7.36 sec. Using the last valid frame instead.\n","  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n","\n"]},{"output_type":"stream","name":"stdout","text":["0.7s matrix 1: 'i' (confidence: 67.01%)\n","0.7s matrix 2: 'i' (confidence: 51.05%)\n","0.7s matrix 3: 'other' (confidence: 87.21%)\n","0.7s matrix 4: 'study' (confidence: 68.62%)\n","0.7s matrix 5: 'study' (confidence: 99.72%)\n","0.7s matrix 6: 'study' (confidence: 99.30%)\n","0.7s matrix 7: 'computer' (confidence: 97.87%)\n","0.7s matrix 8: 'computer' (confidence: 99.86%)\n","0.7s matrix 9: 'science' (confidence: 99.94%)\n","0.7s matrix 10: 'science' (confidence: 97.48%)\n","0.7s matrix 11: 'read' (confidence: 83.73%)\n","\n","Second pass (0.9s chunks):\n","0.9s matrix 1: 'i' (confidence: 82.05%)\n","0.9s matrix 2: 'other' (confidence: 97.12%)\n","0.9s matrix 3: 'read' (confidence: 79.83%)\n","0.9s matrix 4: 'study' (confidence: 99.77%)\n","0.9s matrix 5: 'other' (confidence: 50.17%)\n","0.9s matrix 6: 'computer' (confidence: 99.91%)\n","0.9s matrix 7: 'science' (confidence: 99.50%)\n","0.9s matrix 8: 'science' (confidence: 98.38%)\n","0.9s matrix 9: 'i' (confidence: 66.67%)\n","\n","Full sentence (0.7s chunks): i i other study study study computer computer science science read\n","Full sentence (0.9s chunks): i other read study other computer science science i\n","Final combined sentence: i study computer science\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1_VE0KFP_nVWfbEx-sgNvax5wDlxwLDiN","timestamp":1747676225216},{"file_id":"1hIiVE8MLu3R0wOXjXDZFY7HriLBF7wom","timestamp":1744740036514}],"authorship_tag":"ABX9TyNM9OJNvNLKDFrkXMShHpl0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}